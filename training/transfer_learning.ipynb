{
 "cells": [
  {
   "cell_type": "code",
   "id": "4f48b6a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:20.650832Z",
     "iopub.status.busy": "2025-06-24T04:07:20.650609Z",
     "iopub.status.idle": "2025-06-24T04:07:33.583326Z",
     "shell.execute_reply": "2025-06-24T04:07:33.582718Z"
    },
    "papermill": {
     "duration": 12.93742,
     "end_time": "2025-06-24T04:07:33.584538",
     "exception": false,
     "start_time": "2025-06-24T04:07:20.647118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from PIL import Image\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.model import Student, Teacher"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1894f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.622863Z",
     "iopub.status.busy": "2025-06-24T04:07:33.622668Z",
     "iopub.status.idle": "2025-06-24T04:07:33.713246Z",
     "shell.execute_reply": "2025-06-24T04:07:33.712631Z"
    },
    "papermill": {
     "duration": 0.095314,
     "end_time": "2025-06-24T04:07:33.714287",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.618973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5524d363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.721923Z",
     "iopub.status.busy": "2025-06-24T04:07:33.721553Z",
     "iopub.status.idle": "2025-06-24T04:07:33.728088Z",
     "shell.execute_reply": "2025-06-24T04:07:33.727620Z"
    },
    "papermill": {
     "duration": 0.011279,
     "end_time": "2025-06-24T04:07:33.729126",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.717847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "class RiceSeedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = torch.tensor(self.data.loc[index, \"Label\"]).long()\n",
    "        \n",
    "        path = os.path.join(self.data.loc[index, \"Path\"])\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "transform = {\n",
    "    \"Train\": transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    \n",
    "    \"Validation\": transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    \n",
    "    \"Test\": transforms.Compose([\n",
    "        transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.LANCZOS),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5d97250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.736465Z",
     "iopub.status.busy": "2025-06-24T04:07:33.736065Z",
     "iopub.status.idle": "2025-06-24T04:07:33.844697Z",
     "shell.execute_reply": "2025-06-24T04:07:33.843974Z"
    },
    "papermill": {
     "duration": 0.113555,
     "end_time": "2025-06-24T04:07:33.845835",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.732280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "df = pd.read_csv(r\"/kaggle/input/meta-rice-co-hong/meta_rice_seed.csv\")\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93b2e0ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.853715Z",
     "iopub.status.busy": "2025-06-24T04:07:33.853452Z",
     "iopub.status.idle": "2025-06-24T04:07:33.866101Z",
     "shell.execute_reply": "2025-06-24T04:07:33.865623Z"
    },
    "papermill": {
     "duration": 0.01766,
     "end_time": "2025-06-24T04:07:33.867114",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.849454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 'BC15', 'HuongThom', 'Nep87', 'Q5', 'TBR36', 'TBR45', 'TH3', 'ThienUu8', 'Xi23'\n",
    "\n",
    "name = \"TH3\"\n",
    "\n",
    "sub_df = df.loc[df[\"RiceSeed\"] == name].copy().reset_index(drop=True)\n",
    "df_train = sub_df.loc[sub_df[\"Type\"] == \"Train\"].copy().reset_index(drop=True)\n",
    "df_val = sub_df.loc[sub_df[\"Type\"] == \"Validation\"].copy().reset_index(drop=True)\n",
    "df_test = sub_df.loc[sub_df[\"Type\"] == \"Test\"].copy().reset_index(drop=True)\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "288ecbf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.874995Z",
     "iopub.status.busy": "2025-06-24T04:07:33.874504Z",
     "iopub.status.idle": "2025-06-24T04:07:33.877977Z",
     "shell.execute_reply": "2025-06-24T04:07:33.877318Z"
    },
    "papermill": {
     "duration": 0.008547,
     "end_time": "2025-06-24T04:07:33.879068",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.870521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "folder_path = f''\n",
    "os.makedirs(folder_path, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2549288c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.887129Z",
     "iopub.status.busy": "2025-06-24T04:07:33.886939Z",
     "iopub.status.idle": "2025-06-24T04:07:33.895661Z",
     "shell.execute_reply": "2025-06-24T04:07:33.894861Z"
    },
    "papermill": {
     "duration": 0.013684,
     "end_time": "2025-06-24T04:07:33.896673",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.882989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 100\n",
    "TEMPERATURE1 = 2\n",
    "TEMPERATURE2 = 2\n",
    "ALPHA = 0.9\n",
    "GAMMA = 0.05 \n",
    "BETA = 0.05\n",
    "\n",
    "result = {\n",
    "    \"Optimizer\": \"Adam\",\n",
    "    \"Learning_rate\": LEARNING_RATE,\n",
    "    \"Num_Epochs\": EPOCHS,\n",
    "    \"TEMPERATURE1\": TEMPERATURE1,\n",
    "    \"TEMPERATURE2\": TEMPERATURE2,\n",
    "    \"Alpha\": ALPHA,\n",
    "    \"Gamma\": GAMMA,\n",
    "    \"Beta\": BETA\n",
    "}\n",
    "\n",
    "classes = df_train[\"Label\"].unique()\n",
    "\n",
    "train_dataset = RiceSeedDataset(data=df_train, transform=transform[\"Train\"])\n",
    "valid_dataset = RiceSeedDataset(data=df_val, transform=transform[\"Validation\"])\n",
    "test_dataset = RiceSeedDataset(data=df_test, transform=transform[\"Test\"])\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "data_loader = {\"Train\": train_dataloader, \"Validation\": valid_dataloader}\n",
    "\n",
    "print(classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "316a61d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.904604Z",
     "iopub.status.busy": "2025-06-24T04:07:33.904392Z",
     "iopub.status.idle": "2025-06-24T04:07:33.925321Z",
     "shell.execute_reply": "2025-06-24T04:07:33.924858Z"
    },
    "papermill": {
     "duration": 0.026074,
     "end_time": "2025-06-24T04:07:33.926242",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.900168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "def cross_entropy_loss(logits, labels, criterion):\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    for logit in logits:\n",
    "        loss = criterion(logit, labels)\n",
    "        losses.append(loss)\n",
    "        _, pred = torch.max(logit, dim=1)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return losses, predictions\n",
    "\n",
    "def stage_wise_response_distillation(logit_students, logit_teacher, criterion):\n",
    "    global TEMPERATURE1\n",
    "\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for logit_s, logit_t in zip(logit_students, logit_teacher):\n",
    "        prob_student = F.log_softmax(logit_s / TEMPERATURE1, dim=1)\n",
    "        prob_teacher = F.softmax(logit_t / TEMPERATURE1, dim=1)\n",
    "\n",
    "        loss = criterion(prob_student, prob_teacher) * (TEMPERATURE1**2)\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def stage_wise_channel_distillation(fea_students, fea_teachers):\n",
    "    total_dist = 0.0\n",
    "    for fea_s, fea_t in zip(fea_students, fea_teachers):\n",
    "        dist = F.pairwise_distance(fea_s.view(fea_s.size(0), -1),\n",
    "                                   fea_t.view(fea_t.size(0), -1), p=2)\n",
    "        total_dist += dist.mean()\n",
    "    return total_dist\n",
    "\n",
    "def cross_stage_review_distillation(logits1_student, logits2_student, criterion):\n",
    "    global TEMPERATURE2\n",
    "    \n",
    "    def compute(logit_student, logit_teacher, temperature, criterion):\n",
    "        prob_student = F.log_softmax(logit_student / temperature, dim=1)\n",
    "        prob_teacher = F.softmax(logit_teacher / temperature, dim=1)\n",
    "        loss = criterion(prob_student, prob_teacher) * (temperature**2)\n",
    "        return loss\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    # 1 2\n",
    "    total_loss += compute(logits2_student, logits1_student.detach(), TEMPERATURE2, criterion)\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def train_model(data_loader, model, criterion, optimizer, num_epochs, device, early_stop=True, patience=10):\n",
    "    \n",
    "    global ALPHA, GAMMA, BETA\n",
    "    \n",
    "    student, teacher = model\n",
    "    student, teacher = student.to(device), teacher.to(device)\n",
    "\n",
    "    criterion_ce, criterion_kl = criterion\n",
    "\n",
    "    optimizer_student, optimizer_teacher = optimizer\n",
    "\n",
    "    wait = 0\n",
    "    best_teacher_wts = copy.deepcopy(teacher.state_dict())\n",
    "    best_student_wts = copy.deepcopy(student.state_dict())\n",
    "    best_val_loss_teacher, best_val_loss_student = float(\"inf\"), float(\"inf\")\n",
    "    best_epoch_teacher, best_epoch_student = 0, 0\n",
    "\n",
    "    history_teacher = {\n",
    "        \"Train_Loss1\": [], \"Train_Acc1\": [], \"Validation_Loss1\": [], \"Validation_Acc1\": [],\n",
    "        \"Train_Loss2\": [], \"Train_Acc2\": [], \"Validation_Loss2\": [], \"Validation_Acc2\": [],\n",
    "        \"Time\": []\n",
    "              }\n",
    "\n",
    "    history_student = {\n",
    "        \"Train_Loss1\": [], \"Train_Acc1\": [], \"Validation_Loss1\": [], \"Validation_Acc1\": [],\n",
    "        \"Train_Loss2\": [], \"Train_Acc2\": [], \"Validation_Loss2\": [], \"Validation_Acc2\": [],\n",
    "        \"Time\": []\n",
    "              }\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for phase in [\"Train\", \"Validation\"]:\n",
    "            if phase == \"Train\":\n",
    "                teacher.train()\n",
    "                student.train()\n",
    "            else:\n",
    "                teacher.eval()\n",
    "                student.eval()\n",
    "\n",
    "            running_loss_teacher = [0.0] * 2\n",
    "            running_corrects_teacher = [0] * 2\n",
    "            running_loss_student = [0.0] * 2\n",
    "            running_corrects_student = [0] * 2\n",
    "            total_samples = 0\n",
    "            \n",
    "            for images, labels in tqdm(data_loader[phase], desc=f\"{phase}\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                total_samples += images.size(0)\n",
    "                \n",
    "                fea1_t, logit1_t, fea2_t, logit2_t = None, None, None, None\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"Train\"):\n",
    "                    fea1_t, logit1_t, fea2_t, logit2_t = teacher(images)\n",
    "                    \n",
    "                    losses, predictions = cross_entropy_loss(logits=[logit1_t, logit2_t], \n",
    "                                                             labels=labels, \n",
    "                                                             criterion=criterion_ce)\n",
    "                    total_loss_teacher = sum(losses)\n",
    "                    \n",
    "                    if phase == \"Train\":\n",
    "                        optimizer_teacher.zero_grad()\n",
    "                        total_loss_teacher.backward()\n",
    "                        optimizer_teacher.step()\n",
    "\n",
    "                    for i in range(2):\n",
    "                        running_loss_teacher[i] += losses[i].item() * images.size(0)\n",
    "                        running_corrects_teacher[i] += (predictions[i] == labels).sum().item()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == \"Train\"):\n",
    "                    fea1_s, logit1_s, fea2_s, logit2_s = student(images)\n",
    "                    \n",
    "                    losses, predictions = cross_entropy_loss(logits=[logit1_s, logit2_s], \n",
    "                                                             labels=labels, \n",
    "                                                             criterion=criterion_ce)\n",
    "                    L_CE = sum(losses)\n",
    "                \n",
    "                    L_SCD = stage_wise_channel_distillation(fea_students=[fea1_s, fea2_s], \n",
    "                                                            fea_teachers= [fea1_t.detach(), fea2_t.detach()])\n",
    "                    \n",
    "                    L_SRD = stage_wise_response_distillation(logit_students=[logit1_s, logit2_s], \n",
    "                                                             logit_teacher=[logit1_t.detach(), logit2_t.detach()],\n",
    "                                                             criterion=criterion_kl)\n",
    "\n",
    "                    L_CRD = cross_stage_review_distillation(logits1_student=logit1_s, \n",
    "                                                            logits2_student=logit2_s, \n",
    "                                                            criterion=criterion_kl)\n",
    "                    \n",
    "                    total_loss_student = (BETA * L_SCD) + (GAMMA * L_CRD) + ((1 - ALPHA) * L_SRD) + (ALPHA * L_CE)\n",
    "\n",
    "                    if phase == \"Train\":\n",
    "                        optimizer_student.zero_grad()\n",
    "                        total_loss_student.backward()\n",
    "                        optimizer_student.step()\n",
    "\n",
    "                    for i in range(2):\n",
    "                        running_loss_student[i] += losses[i].item() * images.size(0)\n",
    "                        running_corrects_student[i] += (predictions[i] == labels).sum().item()\n",
    "            \n",
    "            for i in range(2):\n",
    "                epoch_loss_teacher = running_loss_teacher[i] / total_samples\n",
    "                epoch_acc_teacher = running_corrects_teacher[i] / total_samples\n",
    "\n",
    "                epoch_loss_student = running_loss_student[i] / total_samples\n",
    "                epoch_acc_student = running_corrects_student[i] / total_samples\n",
    "\n",
    "                history_teacher[f\"{phase}_Loss{i+1}\"].append(epoch_loss_teacher)\n",
    "                history_teacher[f\"{phase}_Acc{i+1}\"].append(epoch_acc_teacher)\n",
    "\n",
    "                history_student[f\"{phase}_Loss{i+1}\"].append(epoch_loss_student)\n",
    "                history_student[f\"{phase}_Acc{i+1}\"].append(epoch_acc_student)\n",
    "\n",
    "            if phase == \"Validation\":\n",
    "                epoch_loss_s = [loss / total_samples for loss in running_loss_student]\n",
    "                final_branch_val_loss = epoch_loss_s[-1]\n",
    "                \n",
    "                if best_val_loss_student > final_branch_val_loss:\n",
    "                    best_val_loss_student = final_branch_val_loss\n",
    "                    best_student_wts = copy.deepcopy(student.state_dict())\n",
    "                    best_epoch_student = epoch\n",
    "                    wait = 0\n",
    "                else:\n",
    "                    wait += 1\n",
    "                \n",
    "                avg_val_loss = sum([history_teacher[f\"Validation_Loss{i+1}\"][-1] for i in range(2)]) / 2\n",
    "                \n",
    "                if best_val_loss_teacher > avg_val_loss:\n",
    "                    best_val_loss_teacher = avg_val_loss\n",
    "                    best_teacher_wts = copy.deepcopy(teacher.state_dict())\n",
    "                    best_epoch_teacher = epoch\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start\n",
    "        history_teacher[\"Time\"].append(epoch_duration)\n",
    "        history_student[\"Time\"].append(epoch_duration)\n",
    "\n",
    "        print(\"Teacher\")\n",
    "        for i in range(2):\n",
    "            print(f\"Branch {i+1} - Train Loss: {history_teacher[f'Train_Loss{i+1}'][-1]:.4f}, \"\n",
    "                  f\"Train Acc: {history_teacher[f'Train_Acc{i+1}'][-1]:.4f} | \"\n",
    "                  f\"Valid Loss: {history_teacher[f'Validation_Loss{i+1}'][-1]:.4f}, \"\n",
    "                  f\"Valid Acc: {history_teacher[f'Validation_Acc{i+1}'][-1]:.4f}\")\n",
    "        \n",
    "        print(\"Student\")\n",
    "        for i in range(2):\n",
    "            print(f\"Branch {i+1} - Train Loss: {history_student[f'Train_Loss{i+1}'][-1]:.4f}, \"\n",
    "                  f\"Train Acc: {history_student[f'Train_Acc{i+1}'][-1]:.4f} | \"\n",
    "                  f\"Valid Loss: {history_student[f'Validation_Loss{i+1}'][-1]:.4f}, \"\n",
    "                  f\"Valid Acc: {history_student[f'Validation_Acc{i+1}'][-1]:.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} finished in {epoch_duration:.2f}s\")\n",
    "        \n",
    "        if early_stop and wait >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} (no improvement in {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"Training complete in {time_elapsed:.2f}s\")\n",
    "    \n",
    "    student.load_state_dict(best_student_wts)\n",
    "    teacher.load_state_dict(best_teacher_wts)\n",
    "    model = (student, teacher)\n",
    "    history = (pd.DataFrame(history_student), pd.DataFrame(history_teacher))\n",
    "    best_val_loss = (best_val_loss_student, best_val_loss_teacher)\n",
    "    best_epoch = (best_epoch_student, best_epoch_teacher)\n",
    "    \n",
    "    return model, history, time_elapsed, best_val_loss, best_epoch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1e936d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:33.933926Z",
     "iopub.status.busy": "2025-06-24T04:07:33.933511Z",
     "iopub.status.idle": "2025-06-24T04:07:35.939939Z",
     "shell.execute_reply": "2025-06-24T04:07:35.939334Z"
    },
    "papermill": {
     "duration": 2.011541,
     "end_time": "2025-06-24T04:07:35.941207",
     "exception": false,
     "start_time": "2025-06-24T04:07:33.929666",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "student = Student(num_classes=len(classes), num_layers=[4, 6, 8, 10], growth_rate=16)\n",
    "student.load_state_dict(torch.load(\"student_weights.pth\"))\n",
    "\n",
    "teacher = Teacher(len(classes))\n",
    "teacher.load_state_dict(torch.load(\"teacher_weights.pth\"))\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "criterion_kl = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "optimizer_student = torch.optim.Adam(student.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_teacher = torch.optim.Adam(teacher.parameters(), lr=LEARNING_RATE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab3be435",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:07:35.950497Z",
     "iopub.status.busy": "2025-06-24T04:07:35.949874Z",
     "iopub.status.idle": "2025-06-24T04:14:53.412291Z",
     "shell.execute_reply": "2025-06-24T04:14:53.411377Z"
    },
    "papermill": {
     "duration": 437.468069,
     "end_time": "2025-06-24T04:14:53.413472",
     "exception": false,
     "start_time": "2025-06-24T04:07:35.945403",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "model, history, time_elapse, best_val_loss, best_epoch = train_model(data_loader=data_loader, \n",
    "                                                                     model=(student, teacher), \n",
    "                                                                     criterion=(criterion_ce, criterion_kl), \n",
    "                                                                     optimizer=(optimizer_student, optimizer_teacher), \n",
    "                                                                     num_epochs=EPOCHS,\n",
    "                                                                     device=device, \n",
    "                                                                     early_stop=True, \n",
    "                                                                     patience=20)\n",
    "\n",
    "student, teacher = model\n",
    "history_student, history_teacher = history\n",
    "\n",
    "result[\"Best_val_loss_student\"] = best_val_loss[0]\n",
    "result[\"Best_val_loss_teacher\"] = best_val_loss[1]\n",
    "\n",
    "result[\"Best_epoch_student\"] = best_epoch[0]\n",
    "result[\"Best_epoch_teacher\"] = best_epoch[1]\n",
    "\n",
    "result[\"Time_elapse\"] = time_elapse"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2343a546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:14:53.474638Z",
     "iopub.status.busy": "2025-06-24T04:14:53.474371Z",
     "iopub.status.idle": "2025-06-24T04:14:54.269673Z",
     "shell.execute_reply": "2025-06-24T04:14:54.268964Z"
    },
    "papermill": {
     "duration": 0.828275,
     "end_time": "2025-06-24T04:14:54.271721",
     "exception": false,
     "start_time": "2025-06-24T04:14:53.443446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Teacher\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "\n",
    "for i in range(2):\n",
    "    branch_id = i + 1\n",
    "\n",
    "    # Accuracy plot\n",
    "    axs[i, 0].plot(history_teacher[f'Train_Acc{branch_id}'], label='Train Accuracy')\n",
    "    axs[i, 0].plot(history_teacher[f'Validation_Acc{branch_id}'], label='Validation Accuracy')\n",
    "    axs[i, 0].set_xlabel('Epoch')\n",
    "    axs[i, 0].set_ylabel('Accuracy')\n",
    "    axs[i, 0].set_title(f'Branch {branch_id} - Accuracy')\n",
    "    axs[i, 0].legend()\n",
    "\n",
    "    # Loss plot\n",
    "    axs[i, 1].plot(history_teacher[f'Train_Loss{branch_id}'], label='Train Loss')\n",
    "    axs[i, 1].plot(history_teacher[f'Validation_Loss{branch_id}'], label='Validation Loss')\n",
    "    axs[i, 1].set_xlabel('Epoch')\n",
    "    axs[i, 1].set_ylabel('Loss')\n",
    "    axs[i, 1].set_title(f'Branch {branch_id} - Loss')\n",
    "    axs[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6bafbf3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:14:54.337225Z",
     "iopub.status.busy": "2025-06-24T04:14:54.337023Z",
     "iopub.status.idle": "2025-06-24T04:14:55.080343Z",
     "shell.execute_reply": "2025-06-24T04:14:55.079568Z"
    },
    "papermill": {
     "duration": 0.779685,
     "end_time": "2025-06-24T04:14:55.083939",
     "exception": false,
     "start_time": "2025-06-24T04:14:54.304254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Student\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "\n",
    "for i in range(2):\n",
    "    branch_id = i + 1\n",
    "\n",
    "    # Accuracy plot\n",
    "    axs[i, 0].plot(history_student[f'Train_Acc{branch_id}'], label='Train Accuracy')\n",
    "    axs[i, 0].plot(history_student[f'Validation_Acc{branch_id}'], label='Validation Accuracy')\n",
    "    axs[i, 0].set_xlabel('Epoch')\n",
    "    axs[i, 0].set_ylabel('Accuracy')\n",
    "    axs[i, 0].set_title(f'Branch {branch_id} - Accuracy')\n",
    "    axs[i, 0].legend()\n",
    "\n",
    "    # Loss plot\n",
    "    axs[i, 1].plot(history_student[f'Train_Loss{branch_id}'], label='Train Loss')\n",
    "    axs[i, 1].plot(history_student[f'Validation_Loss{branch_id}'], label='Validation Loss')\n",
    "    axs[i, 1].set_xlabel('Epoch')\n",
    "    axs[i, 1].set_ylabel('Loss')\n",
    "    axs[i, 1].set_title(f'Branch {branch_id} - Loss')\n",
    "    axs[i, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20e4b02f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:14:55.161414Z",
     "iopub.status.busy": "2025-06-24T04:14:55.160893Z",
     "iopub.status.idle": "2025-06-24T04:15:00.085523Z",
     "shell.execute_reply": "2025-06-24T04:15:00.084535Z"
    },
    "papermill": {
     "duration": 4.963519,
     "end_time": "2025-06-24T04:15:00.087179",
     "exception": false,
     "start_time": "2025-06-24T04:14:55.123660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# teacher\n",
    "teacher.eval()\n",
    "\n",
    "branch_outputs = {\n",
    "    '1': {'correct': [], 'predict': []},\n",
    "    '2': {'correct': [], 'predict': []},\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        fea1, logit1, fea2, logit2 = teacher(images)\n",
    "\n",
    "        for i, outputs in enumerate([logit1, logit2], start=1):\n",
    "            _, predicts = torch.max(outputs, dim=1)\n",
    "            branch_outputs[str(i)]['correct'].extend(labels.cpu().numpy())\n",
    "            branch_outputs[str(i)]['predict'].extend(predicts.cpu().numpy())\n",
    "\n",
    "for i in range(1, 3):\n",
    "    correct = branch_outputs[str(i)]['correct']\n",
    "    predict = branch_outputs[str(i)]['predict']\n",
    "    \n",
    "    acc = accuracy_score(correct, predict)\n",
    "    precision = precision_score(correct, predict, average='weighted')\n",
    "    recall = recall_score(correct, predict, average='weighted')\n",
    "    f1 = f1_score(correct, predict, average='weighted')\n",
    "\n",
    "    result[f\"Teacher_Branch_{i}_Acc\"] = acc\n",
    "    result[f\"Teacher_Branch_{i}_Pre\"] = precision\n",
    "    result[f\"Teacher_Branch_{i}_Rec\"] = recall\n",
    "    result[f\"Teacher_Branch_{i}_F1\"] = f1\n",
    "    \n",
    "    print(f\"\\n=== Branch {i} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "\n",
    "    report = classification_report(correct, predict, target_names=labels, digits=4)\n",
    "\n",
    "    path = os.path.join(folder_path, f\"classification_report_teacher_branch_{i}\")\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(correct, predict)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - Branch {i}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(folder_path, f'confusion_matrix_teacher_branch_{i}.png'))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f411a95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:15:00.176026Z",
     "iopub.status.busy": "2025-06-24T04:15:00.175767Z",
     "iopub.status.idle": "2025-06-24T04:15:02.992550Z",
     "shell.execute_reply": "2025-06-24T04:15:02.991916Z"
    },
    "papermill": {
     "duration": 2.85499,
     "end_time": "2025-06-24T04:15:02.993869",
     "exception": false,
     "start_time": "2025-06-24T04:15:00.138879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# student\n",
    "student.eval()\n",
    "\n",
    "branch_outputs = {\n",
    "    '1': {'correct': [], 'predict': []},\n",
    "    '2': {'correct': [], 'predict': []},\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        fea1, logit1, fea2, logit2 = student(images)\n",
    "\n",
    "        for i, outputs in enumerate([logit1, logit2], start=1):\n",
    "            _, predicts = torch.max(outputs, dim=1)\n",
    "            branch_outputs[str(i)]['correct'].extend(labels.cpu().numpy())\n",
    "            branch_outputs[str(i)]['predict'].extend(predicts.cpu().numpy())\n",
    "\n",
    "for i in range(1, 3):\n",
    "    correct = branch_outputs[str(i)]['correct']\n",
    "    predict = branch_outputs[str(i)]['predict']\n",
    "    \n",
    "    acc = accuracy_score(correct, predict)\n",
    "    precision = precision_score(correct, predict, average='weighted')\n",
    "    recall = recall_score(correct, predict, average='weighted')\n",
    "    f1 = f1_score(correct, predict, average='weighted')\n",
    "\n",
    "    result[f\"Student_Branch_{i}_Acc\"] = acc\n",
    "    result[f\"Student_Branch_{i}_Pre\"] = precision\n",
    "    result[f\"Student_Branch_{i}_Rec\"] = recall\n",
    "    result[f\"Student_Branch_{i}_F1\"] = f1\n",
    "    \n",
    "    print(f\"\\n=== Branch {i} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "\n",
    "    # Save classification report\n",
    "    report = classification_report(correct, predict, target_names=labels, digits=4)\n",
    "\n",
    "    path = os.path.join(folder_path, f\"classification_report_student_branch_{i}\")\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(correct, predict)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=labels,\n",
    "                yticklabels=labels)\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - Branch {i}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(os.path.join(folder_path, f'confusion_matrix_student_branch_{i}.png'))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "953b4bce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T04:15:03.070611Z",
     "iopub.status.busy": "2025-06-24T04:15:03.069938Z",
     "iopub.status.idle": "2025-06-24T04:15:03.247498Z",
     "shell.execute_reply": "2025-06-24T04:15:03.246542Z"
    },
    "papermill": {
     "duration": 0.216309,
     "end_time": "2025-06-24T04:15:03.249209",
     "exception": false,
     "start_time": "2025-06-24T04:15:03.032900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "result = pd.DataFrame([result])\n",
    "result.to_csv(os.path.join(folder_path, \"result.csv\"), index=False)\n",
    "\n",
    "history_student.to_csv(os.path.join(folder_path, \"history_student.csv\"), index=False)\n",
    "history_teacher.to_csv(os.path.join(folder_path, \"history_teacher.csv\"), index=False)\n",
    "\n",
    "\n",
    "torch.save(student.state_dict(), os.path.join(folder_path, f'student_weights.pth'))\n",
    "torch.save(teacher.state_dict(), os.path.join(folder_path, f'teacher_weights.pth'))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7513075,
     "sourceId": 11950383,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7524230,
     "sourceId": 11965760,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7723127,
     "sourceId": 12256579,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 470.59506,
   "end_time": "2025-06-24T04:15:06.582232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-24T04:07:15.987172",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
